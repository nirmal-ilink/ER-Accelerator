Timestamp,User,Action,Module,Status,Details
2026-01-27 16:52:05,admin@icore.com,Access Control Review,Governance,Success,Updated schema definition for AWS Redshift.Inventory
2026-01-27 05:25:10,j.doe@client.com,Compliance Check,Inspector,Failed,Null pointer exception during Compliance Check
2026-01-26 22:36:59,data_steward_1,Rule validation,Quality,Success,Lineage graph refreshed for Google BigQuery pipeline
2026-01-26 15:04:25,etl_service_account,Data Quality Check,Profiler,Success,Updated schema definition for Databricks.Inventory
2026-01-26 09:32:59,System,Description Update,Catalogue,Success,Updated schema definition for SAP.Inventory
2026-01-26 09:07:07,etl_service_account,Rule validation,Quality,Success,Successfully processed 152 objects from Google BigQuery
2026-01-25 14:03:35,j.doe@client.com,Glossary Link,Catalogue,Failed,Null pointer exception during Glossary Link
2026-01-25 11:55:39,admin@icore.com,Metadata Sync,Connector,Success,Validated 50 data quality rules
2026-01-25 02:59:34,System,Connection Test,Connector,Warning,High latency observed during Connection Test
2026-01-23 03:49:12,admin@icore.com,Glossary Link,Catalogue,Failed,API rate limit exceeded for Azure Synapse
2026-01-22 14:10:58,j.doe@client.com,Anomaly Detection,Quality,Failed,Schema mismatch detected in Google BigQuery source
2026-01-22 09:47:41,etl_service_account,Description Update,Catalogue,Success,Lineage graph refreshed for AWS Redshift pipeline
2026-01-22 05:42:23,etl_service_account,Dependency Check,Lineage,Success,Metadata sync completed in 1.53s
2026-01-22 03:58:00,etl_service_account,Metadata Sync,Connector,Success,Updated schema definition for Salesforce.Inventory
2026-01-21 21:41:38,j.doe@client.com,Connection Test,Connector,Success,Metadata sync completed in 4.89s
2026-01-21 03:45:14,etl_service_account,Anomaly Detection,Quality,Success,Lineage graph refreshed for Snowflake pipeline
2026-01-19 17:02:03,m.smith@icore.com,PII Detection,Inspector,Warning,Partial data load: 5 records skipped
2026-01-19 04:23:20,j.doe@client.com,Graph Build,Lineage,Success,Lineage graph refreshed for AWS Redshift pipeline
2026-01-18 07:01:35,System,Rule validation,Quality,Success,Successfully processed 494 objects from Salesforce
2026-01-17 15:35:58,j.doe@client.com,Pattern Analysis,Profiler,Success,Successfully processed 217 objects from AWS Redshift
2026-01-17 10:50:13,admin@icore.com,Policy Update,Governance,Success,Successfully processed 398 objects from Snowflake
2026-01-16 21:40:35,j.doe@client.com,Policy Update,Governance,Warning,Partial data load: 5 records skipped
2026-01-16 04:04:23,etl_service_account,Compliance Check,Inspector,Warning,Scan completed with 3 warnings on Databricks tables
2026-01-15 09:57:11,System,Null Check,Profiler,Success,Updated schema definition for Databricks.Inventory
2026-01-15 05:50:27,System,Connection Test,Connector,Warning,Deprecated API version usage detected for Oracle
2026-01-14 09:42:34,m.smith@icore.com,Anomaly Detection,Quality,Failed,Connection timeout to Oracle instance (Error 504)
2026-01-14 04:38:06,data_steward_1,Tag Assignment,Governance,Warning,PII potential match found in unencrypted field
2026-01-14 04:23:23,data_steward_1,Incremental Load,Connector,Warning,Deprecated API version usage detected for AWS Redshift
2026-01-14 02:02:33,data_steward_1,Pattern Analysis,Profiler,Success,Validated 26 data quality rules
2026-01-13 16:45:12,m.smith@icore.com,Policy Update,Governance,Success,Successfully processed 47 objects from Salesforce
2026-01-12 07:20:53,etl_service_account,Policy Update,Governance,Success,Metadata sync completed in 2.40s
2026-01-11 09:31:48,etl_service_account,Pattern Analysis,Profiler,Failed,Connection timeout to SAP instance (Error 504)
2026-01-11 06:32:26,data_steward_1,Dependency Check,Lineage,Failed,Authentication failed for user data_steward_1
2026-01-10 04:02:06,admin@icore.com,Graph Build,Lineage,Warning,High latency observed during Graph Build
2026-01-09 05:42:29,admin@icore.com,Metadata Sync,Connector,Failed,Authentication failed for user admin@icore.com
2026-01-08 18:12:26,etl_service_account,Incremental Load,Connector,Success,Successfully processed 292 objects from Google BigQuery
2026-01-08 14:34:14,System,Policy Update,Governance,Success,Metadata sync completed in 1.64s
2026-01-08 10:35:40,data_steward_1,Incremental Load,Connector,Success,Lineage graph refreshed for Azure Synapse pipeline
2026-01-07 01:06:15,System,Access Control Review,Governance,Success,Metadata sync completed in 0.67s
2026-01-05 18:49:51,data_steward_1,Anomaly Detection,Quality,Success,Validated 22 data quality rules
2026-01-05 01:18:26,data_steward_1,Sensitive Data Scan,Inspector,Failed,API rate limit exceeded for AWS Redshift
2026-01-04 10:02:26,admin@icore.com,Tag Assignment,Governance,Warning,PII potential match found in unencrypted field
2026-01-04 09:06:10,etl_service_account,Graph Build,Lineage,Warning,High latency observed during Graph Build
2026-01-04 00:19:30,etl_service_account,Graph Build,Lineage,Success,Successfully processed 30 objects from Snowflake
2026-01-03 23:53:12,j.doe@client.com,Metadata Sync,Connector,Success,Updated schema definition for Salesforce.Inventory
2026-01-03 15:17:36,j.doe@client.com,Anomaly Detection,Quality,Success,Validated 6 data quality rules
2026-01-02 04:36:08,m.smith@icore.com,Pattern Analysis,Profiler,Success,Successfully processed 249 objects from SAP
2026-01-01 23:38:53,admin@icore.com,Rule validation,Quality,Success,Lineage graph refreshed for Oracle pipeline
2025-12-30 01:15:39,admin@icore.com,Sensitive Data Scan,Inspector,Warning,PII potential match found in unencrypted field
2025-12-29 00:46:11,j.doe@client.com,Null Check,Profiler,Success,Lineage graph refreshed for Oracle pipeline
2026-01-27 21:56:33,admin,User Login,Auth,Success,Session started for admin
2026-01-27 22:01:41,admin,User Login,Auth,Success,Session started for admin
2026-01-28 17:02:49,admin,User Login,Auth,Success,Session started for admin
2026-01-29 14:35:48,admin,User Login,Auth,Success,Session started for admin
2026-01-30 11:38:54,admin,User Login,Auth,Success,Session started for admin
2026-01-30 12:19:24,admin,User Login,Auth,Success,Session started for admin
2026-01-30 12:22:10,Administrator,Match Approved,Match Review,Success,Cluster CL005 resolved by Administrator
2026-01-30 12:45:12,admin,User Login,Auth,Success,Session started for admin
2026-01-30 13:19:51,admin,User Login,Auth,Success,Session started for admin
2026-01-30 13:20:41,admin,User Login,Auth,Success,Session started for admin
2026-01-30 14:48:59,admin,User Login,Auth,Success,Session started for admin
2026-01-30 22:57:09,admin,User Login,Auth,Success,Session started for admin
2026-01-31 00:47:39,admin,User Login,Auth,Success,Session started for admin
2026-01-31 16:58:39,admin,User Login,Auth,Success,Session started for admin
2026-01-31 17:04:11,admin,User Login,Auth,Success,Session started for admin
2026-01-31 17:09:49,admin,User Login,Auth,Success,Session started for admin
2026-02-02 13:37:08,admin,User Login,Auth,Success,Session started for admin
2026-02-02 14:01:37,admin,User Login,Auth,Success,Session started for admin
2026-02-02 14:08:48,admin,User Login,Auth,Success,Session started for admin
2026-02-02 14:11:33,admin,User Login,Auth,Success,Session started for admin
2026-02-02 15:53:59,admin,User Login,Auth,Success,Session started for admin
2026-02-02 16:01:32,admin,User Login,Auth,Success,Session started for admin
2026-02-02 16:02:34,admin,User Login,Auth,Success,Session started for admin
2026-02-02 16:03:20,admin,User Login,Auth,Success,Session started for admin
2026-02-02 16:32:09,admin,User Login,Auth,Success,Session started for admin
2026-02-02 17:07:20,admin,User Login,Auth,Success,Session started for admin
2026-02-02 19:02:19,admin,User Login,Auth,Success,Session started for admin
2026-02-02 19:02:43,admin,User Login,Auth,Success,Session started for admin
2026-02-03 11:39:08,admin,User Login,Auth,Success,Session started for admin
2026-02-03 11:43:42,admin,User Login,Auth,Success,Session started for admin
2026-02-03 11:55:31,admin,User Login,Auth,Success,Session started for admin
2026-02-03 12:01:56,admin,User Login,Auth,Success,Session started for admin
2026-02-03 12:11:29,admin,User Login,Auth,Success,Session started for admin
2026-02-03 12:18:58,admin,User Login,Auth,Success,Session started for admin
2026-02-03 12:56:56,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:21:25,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:27:12,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:31:34,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:38:32,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:40:18,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:44:29,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:51:34,admin,User Login,Auth,Success,Session started for admin
2026-02-03 14:59:22,admin,User Login,Auth,Success,Session started for admin
2026-02-03 15:04:17,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: a68938f8-1d06-451e-b6de-f0d24a871806)
2026-02-03 15:26:50,admin,User Login,Auth,Success,Session started for admin
2026-02-03 15:28:46,admin,User Login,Auth,Success,Session started for admin
2026-02-03 16:28:50,admin,User Login,Auth,Success,Session started for admin
2026-02-03 16:51:55,admin,User Login,Auth,Success,Session started for admin
2026-02-03 16:55:49,admin,User Login,Auth,Success,Session started for admin
2026-02-03 17:05:15,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: 7e70c07a-6875-4737-ba9a-92338a8084a2)
2026-02-03 17:11:47,admin,User Login,Auth,Success,Session started for admin
2026-02-03 20:39:57,admin,User Login,Auth,Success,Session started for admin
2026-02-03 20:48:57,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: f6ae4ce0-845a-48fe-9923-b1ba910d180b)
2026-02-03 21:14:14,admin,User Login,Auth,Success,Session started for admin
2026-02-03 21:54:24,Administrator,User Logout,Auth,Success,User signed out manually
2026-02-03 21:54:34,admin,User Login,Auth,Success,Session started for admin
2026-02-03 21:59:43,System,Saved Configuration,Connectors,Failed,Failed to save databricks configuration: [NO_ACTIVE_SESSION] No active Spark session found. Please create a new Spark session before running the code.
2026-02-03 22:02:31,admin,User Login,Auth,Success,Session started for admin
2026-02-03 22:06:07,admin,User Login,Auth,Success,Session started for admin
2026-02-03 22:10:31,Administrator,Match Approved,Match Review,Success,Cluster CL006 resolved by Administrator
2026-02-03 22:31:06,admin,User Login,Auth,Success,Session started for admin
2026-02-03 22:47:29,admin,User Login,Auth,Success,Session started for admin
2026-02-03 22:53:02,Administrator,Match Approved,Match Review,Success,Cluster CL007 resolved by Administrator
2026-02-03 22:53:22,Administrator,Match Approved,Match Review,Success,Cluster CL008 resolved by Administrator
2026-02-04 11:47:26,admin,User Login,Auth,Success,Session started for admin
2026-02-04 12:41:15,admin,User Login,Auth,Success,Session started for admin
2026-02-04 12:45:27,admin,User Login,Auth,Success,Session started for admin
2026-02-04 12:46:04,admin,User Login,Auth,Success,Session started for admin
2026-02-04 13:03:12,admin,User Login,Auth,Success,Session started for admin
2026-02-04 13:43:05,admin,User Login,Auth,Success,Session started for admin
2026-02-04 13:43:26,admin,User Login,Auth,Success,Session started for admin
2026-02-04 13:44:03,admin,User Login,Auth,Success,Session started for admin
2026-02-04 13:48:12,admin,User Login,Auth,Success,Session started for admin
2026-02-04 13:49:08,admin,User Login,Auth,Success,Session started for admin
2026-02-04 14:09:56,admin,User Login,Auth,Success,Session started for admin
2026-02-04 14:10:53,admin,User Login,Auth,Success,Session started for admin
2026-02-04 15:51:49,admin,User Login,Auth,Success,Session started for admin
2026-02-04 15:54:35,admin,User Login,Auth,Success,Session started for admin
2026-02-05 09:02:19,admin,User Login,Auth,Success,Session started for admin
2026-02-05 09:02:54,admin,User Login,Auth,Success,Session started for admin
2026-02-05 09:04:14,admin,User Login,Auth,Success,Session started for admin
2026-02-05 09:08:05,System,Saved Configuration,Connectors,Success,Saved sqlserver configuration 'New Saved Config' (ID: 6f02cee5-64c7-4e59-a88b-33fef9eb5d3a)
2026-02-05 09:08:42,admin,User Login,Auth,Success,Session started for admin
2026-02-05 09:09:13,admin,User Login,Auth,Success,Session started for admin
2026-02-05 09:11:51,admin,User Login,Auth,Success,Session started for admin
2026-02-05 17:27:05,admin,User Login,Auth,Success,Session started for admin
2026-02-05 17:35:38,admin,User Login,Auth,Success,Session started for admin
2026-02-05 17:38:20,admin,User Login,Auth,Success,Session started for admin
2026-02-06 09:14:17,admin,User Login,Auth,Success,Session started for admin
2026-02-06 09:18:00,admin,User Login,Auth,Success,Session started for admin
2026-02-06 09:19:11,admin,User Login,Auth,Success,Session started for admin
2026-02-06 11:11:34,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: bc68ef4d-b27b-4d20-a8ae-96d328ab02cd) after session recovery
2026-02-06 11:14:05,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: 35b75236-f1cc-40e4-a2ba-94b9046d07ed)
2026-02-06 11:33:49,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: 6f392e4a-de02-418a-bf75-04d98d340604)
2026-02-06 11:35:03,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: f8d32bb8-577f-4649-923e-47d371b212cc)
2026-02-06 11:37:15,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: 55320a99-b8b9-45aa-ab0f-33878d879086)
2026-02-06 11:39:16,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: 5af68e21-b5b1-4050-bb88-33ec7ea9118a)
2026-02-06 11:39:29,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: d9aa1a3b-2e70-4f6d-8334-dc9da8656121)
2026-02-06 11:41:11,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: e4e5228b-7d15-408a-80e7-938279ca8ddc)
2026-02-06 11:43:32,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: e0bc88a6-d2d0-4799-9f94-439ba7403409)
2026-02-06 11:45:13,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: a2e4b537-e7f6-4c56-9f76-46e8d908dafc)
2026-02-06 11:59:25,System,Saved Configuration,Connectors,Success,Saved databricks configuration 'Databricks' (ID: 5622e22c-981e-438b-91ba-a947a4ce1aa7)
2026-02-06 12:15:17,admin,User Login,Auth,Success,Session started for admin
2026-02-06 12:32:54,admin,User Login,Auth,Success,Session started for admin
2026-02-06 12:33:38,admin,User Login,Auth,Success,Session started for admin
2026-02-06 12:37:15,admin,User Login,Auth,Success,Session started for admin
2026-02-06 12:37:45,admin,User Login,Auth,Success,Session started for admin
2026-02-06 13:40:31,admin,User Login,Auth,Success,Session started for admin
2026-02-06 13:40:55,admin,User Login,Auth,Success,Session started for admin
2026-02-06 13:46:10,admin,User Login,Auth,Success,Session started for admin
2026-02-06 14:30:06,admin,User Login,Auth,Success,Session started for admin
2026-02-06 14:30:32,System,Triggered Ingestion,Connectors,Failed,Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: 'DBUtils' object has no attribute 'notebook'
2026-02-06 14:39:15,System,Triggered Ingestion,Connectors,Failed,Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: 'DBUtils' object has no attribute 'notebook'
2026-02-06 14:41:13,System,Triggered Ingestion,Connectors,Failed,Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: 'DBUtils' object has no attribute 'notebook'
2026-02-06 14:47:01,admin,User Login,Auth,Success,Session started for admin
2026-02-06 14:54:58,admin,User Login,Auth,Success,Session started for admin
2026-02-06 14:57:20,admin,User Login,Auth,Success,Session started for admin
2026-02-06 14:57:54,admin,User Login,Auth,Success,Session started for admin
2026-02-06 15:06:58,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-06 15:08:26,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-06 15:09:01,admin,User Login,Auth,Success,Session started for admin
2026-02-06 15:10:26,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 15:13:34,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 15:13:45,admin,User Login,Auth,Success,Session started for admin
2026-02-06 15:14:36,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 15:57:08,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 15:59:49,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:00:02,admin,User Login,Auth,Success,Session started for admin
2026-02-06 16:01:41,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:03:43,admin,User Login,Auth,Success,Session started for admin
2026-02-06 16:05:26,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:10:14,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:11:08,admin,User Login,Auth,Success,Session started for admin
2026-02-06 16:12:34,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:18:45,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:21:57,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:26:10,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task ingestion_task failed with message: Workload failed, see run output for details."
2026-02-06 16:35:06,admin,User Login,Auth,Success,Session started for admin
2026-02-06 16:37:18,admin,User Login,Auth,Success,Session started for admin
2026-02-06 16:42:42,admin,User Login,Auth,Success,Session started for admin
2026-02-06 16:43:54,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-06 16:54:10,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-06 16:54:56,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-06 16:59:00,admin,User Login,Auth,Success,Session started for admin
2026-02-06 17:04:14,admin,User Login,Auth,Success,Session started for admin
2026-02-06 17:09:05,admin,User Login,Auth,Success,Session started for admin
2026-02-06 17:11:36,admin,User Login,Auth,Success,Session started for admin
2026-02-08 22:59:59,admin,User Login,Auth,Success,Session started for admin
2026-02-09 16:14:34,admin,User Login,Auth,Success,Session started for admin
2026-02-09 16:22:28,admin,User Login,Auth,Success,Session started for admin
2026-02-09 16:23:56,System,Saved Configuration (Async),Connectors,Success,Saved databricks configuration 'Databricks' (ID: c768fcbe-e0a3-4383-bde5-e37f56665670)
2026-02-10 09:25:25,admin,User Login,Auth,Success,Session started for admin
2026-02-10 09:28:44,Administrator,User Logout,Auth,Success,User signed out manually
2026-02-10 09:29:28,dev,User Login,Auth,Success,Session started for dev
2026-02-10 09:29:32,Tech Lead,User Logout,Auth,Success,User signed out manually
2026-02-10 09:29:39,exec,User Login,Auth,Success,Session started for exec
2026-02-10 09:29:43,C-Suite User,User Logout,Auth,Success,User signed out manually
2026-02-10 09:29:48,admin,User Login,Auth,Success,Session started for admin
2026-02-10 09:29:52,Administrator,User Logout,Auth,Success,User signed out manually
2026-02-10 09:55:57,admin,User Login,Auth,Success,Session started for admin
2026-02-10 12:41:07,admin,User Login,Auth,Success,Session started for admin
2026-02-10 14:11:05,admin,User Login,Auth,Success,Session started for admin
2026-02-10 15:07:40,admin,User Login,Auth,Success,Session started for admin
2026-02-10 15:19:25,System,Saved Configuration (Async),Connectors,Success,Saved databricks configuration 'Databricks' (ID: 9b0fba6e-39dd-41f7-acfa-8de7e77ba3d4) after session recovery
2026-02-10 15:51:09,Administrator,User Logout,Auth,Success,User signed out manually
2026-02-10 15:52:18,System,Saved Configuration (Async),Connectors,Success,Saved databricks configuration 'Databricks' (ID: 10a2ed47-b4a7-4987-b1b7-6b8296396294) after session recovery
2026-02-11 11:21:49,admin,User Login,Auth,Success,Session started for admin
2026-02-11 11:45:43,System,Saved Configuration (Async),Connectors,Success,Saved sqlserver configuration 'SQL Server' (ID: b6e9a11d-1e17-4fa6-995c-5a0d079e6c57)
2026-02-11 11:45:50,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-11 11:51:41,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-11 11:52:09,admin,User Login,Auth,Success,Session started for admin
2026-02-11 11:53:40,System,Triggered Ingestion,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_brz_ingestion: One of job_cluster_key, new_cluster, or existing_cluster_id must be specified. Serverless compute for workflows is not enabled in the workspace."
2026-02-11 11:56:00,admin,User Login,Auth,Success,Session started for admin
2026-02-11 11:56:51,admin,User Login,Auth,Success,Session started for admin
2026-02-11 11:59:27,System,Triggered Ingestion,Connectors,Success,Triggered /Shared/ER_aligned/nb_brz_ingestion for ID b6e9a11d-1e17-4fa6-995c-5a0d079e6c57. Run ID: 551610944641654
2026-02-11 12:00:57,System,Triggered Profiling,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_mdm_profiling: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task profiling_task failed with message: Workload failed, see run output for details."
2026-02-11 12:07:14,System,Triggered Profiling,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_mdm_profiling: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task profiling_task failed with message: Workload failed, see run output for details."
2026-02-11 12:25:34,admin,User Login,Auth,Success,Session started for admin
2026-02-11 12:26:25,System,Triggered Profiling,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_mdm_profiling: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task profiling_task failed with message: Workload failed, see run output for details."
2026-02-11 12:31:52,System,Triggered Profiling,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_mdm_profiling: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task profiling_task failed with message: Workload failed, see run output for details."
2026-02-11 12:38:38,admin,User Login,Auth,Success,Session started for admin
2026-02-11 12:39:14,System,Triggered Profiling,Connectors,Failed,"Failed to trigger /Shared/ER_aligned/nb_mdm_profiling: failed to reach TERMINATED or SKIPPED, got RunLifeCycleState.INTERNAL_ERROR: Task profiling_task failed with message: Workload failed, see run output for details."
2026-02-11 14:52:36,admin,User Login,Auth,Success,Session started for admin
2026-02-11 15:42:06,admin,User Login,Auth,Success,Session started for admin
2026-02-11 16:35:07,admin,User Login,Auth,Success,Session started for admin
2026-02-11 16:36:37,Administrator,Saved Configuration (Async),Connectors,Failed,"Failed to save sqlserver configuration: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `connection_name` cannot be resolved. Did you mean one of the following? [`connection_id`, `source_type`, `source_name`, `configuration`, `selected_tables`]. SQLSTATE: 42703; line 2 pos 12

JVM stacktrace:
org.apache.spark.sql.AnalysisException
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:681)
	at org.apache.spark.sql.catalyst.analysis.ResolveInsertionBase.$anonfun$createProjectForByNameQuery$2(ResolveInsertionBase.scala:50)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveInsertionBase.$anonfun$createProjectForByNameQuery$1(ResolveInsertionBase.scala:49)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.analysis.ResolveInsertionBase.createProjectForByNameQuery(ResolveInsertionBase.scala:46)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$$anonfun$apply$14.applyOrElse(Analyzer.scala:2058)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$$anonfun$apply$14.applyOrElse(Analyzer.scala:1984)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:47)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:47)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$.apply(Analyzer.scala:1984)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$.apply(Analyzer.scala:1982)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:383)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:484)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:806)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1012)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1012)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1671)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1005)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1002)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1002)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1001)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:989)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1000)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:999)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:475)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:474)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:513)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:401)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1249)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1249)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:941)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:901)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3964)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3792)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3577)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:397)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:283)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:240)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
	at org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:240)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)
	at com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:602)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:602)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:601)"
2026-02-11 17:22:20,admin,User Login,Auth,Success,Session started for admin
2026-02-11 17:42:06,admin,User Login,Auth,Success,Session started for admin
2026-02-11 17:50:12,admin,User Login,Auth,Success,Session started for admin
2026-02-11 17:51:07,admin,User Login,Auth,Success,Session started for admin
2026-02-11 18:12:22,admin,User Login,Auth,Success,Session started for admin
2026-02-11 18:55:15,admin,User Login,Auth,Success,Session started for admin
2026-02-11 19:29:36,Administrator,Saved Configuration (Async),Connectors,Failed,Failed to save sqlserver configuration: RESOURCE_DOES_NOT_EXIST: No cluster found matching: 0211-122641-gsbakciq
2026-02-11 19:40:47,admin,User Login,Auth,Success,Session started for admin
2026-02-11 19:48:40,admin,User Login,Auth,Success,Session started for admin
2026-02-11 19:54:22,admin,User Login,Auth,Success,Session started for admin
2026-02-11 19:57:30,admin,User Login,Auth,Success,Session started for admin
2026-02-11 19:58:16,Administrator,Saved Configuration (Async),Connectors,Failed,"Failed to save sqlserver configuration: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `connection_name` cannot be resolved. Did you mean one of the following? [`connection_id`, `source_type`, `source_name`, `configuration`, `selected_tables`]. SQLSTATE: 42703; line 2 pos 12

JVM stacktrace:
org.apache.spark.sql.AnalysisException
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:681)
	at org.apache.spark.sql.catalyst.analysis.ResolveInsertionBase.$anonfun$createProjectForByNameQuery$2(ResolveInsertionBase.scala:50)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveInsertionBase.$anonfun$createProjectForByNameQuery$1(ResolveInsertionBase.scala:49)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.analysis.ResolveInsertionBase.createProjectForByNameQuery(ResolveInsertionBase.scala:46)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$$anonfun$apply$14.applyOrElse(Analyzer.scala:2063)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$$anonfun$apply$14.applyOrElse(Analyzer.scala:1989)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$.apply(Analyzer.scala:1989)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveInsertInto$.apply(Analyzer.scala:1987)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:668)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:668)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:667)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:640)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:472)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:640)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:557)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:418)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:867)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:830)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3898)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3725)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3523)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:290)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:247)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:536)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:536)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
	at org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:535)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:247)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)
	at com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:595)"
2026-02-11 20:18:06,admin,User Login,Auth,Success,Session started for admin
2026-02-11 20:22:18,admin,User Login,Auth,Success,Session started for admin
2026-02-11 20:23:20,admin,User Login,Auth,Success,Session started for admin
2026-02-11 20:24:12,admin,User Login,Auth,Success,Session started for admin
2026-02-11 20:30:57,Administrator,User Logout,Auth,Success,User signed out manually
2026-02-11 20:31:02,admin,User Login,Auth,Success,Session started for admin
2026-02-11 20:36:16,Administrator,User Logout,Auth,Success,User signed out manually
2026-02-11 20:40:02,admin,User Login,Auth,Success,Session started for admin
2026-02-12 10:51:35,admin,User Login,Auth,Success,Session started for admin
